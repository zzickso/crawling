{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db23179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹페이지에서 필요한 데이터를 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html =      \"<html> \\\n",
    "                <body> \\\n",
    "                        <h1 id='title'>[1]크롤링이란?</h1> \\\n",
    "                        <p class='cssstyle'>웹페이지에서 필요한 데이터를 추출하는 것</p> \\\n",
    "                        <p id='body' align='center'>파이썬을 중심으로 다양한 웹크롤링 기술 발달</p> \\\n",
    "                </body> \\\n",
    "            </html>\"\n",
    "soup = BeautifulSoup(html,\n",
    "\"html.parser\")\n",
    "# 태그로 검색 방법\n",
    "data = soup.find('p')\n",
    "#data = soup.find('p', class_='cssstyle')\n",
    "#data = soup.find('p', 'cssstyle')\n",
    "#data = soup.find('p', attrs={'align':'center'})\n",
    "#data = soup.find(id='body')\n",
    "print(data.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6de10ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹페이지에서 필요한 데이터를 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "data = soup.find('p', class_='cssstyle')\n",
    "print(data.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6cc49a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹페이지에서 필요한 데이터를 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "data = soup.find('p', 'cssstyle')\n",
    "print(data.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488276ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n"
     ]
    }
   ],
   "source": [
    "data = soup.find('p', attrs={'align':'center'})\n",
    "print(data.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5036828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]크롤링이란?\n"
     ]
    }
   ],
   "source": [
    "data = soup.find(id='title')\n",
    "print(data.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99e7281a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]크롤링이란?\n"
     ]
    }
   ],
   "source": [
    "paragraph_data = soup.find_all('h1')\n",
    "\n",
    "for paragraph in paragraph_data:\n",
    "    print(paragraph.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ecdfc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹페이지에서 필요한 데이터를 추출하는 것\n",
      "파이썬을 중심으로 다양한 웹크롤링 기술 발달\n"
     ]
    }
   ],
   "source": [
    "paragraph_data = soup.find_all('p')\n",
    "\n",
    "for paragraph in paragraph_data:\n",
    "    print(paragraph.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fb70cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 기사에 대해 어떻게 생각하시나요?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# 1) reqeusts 라이브러리를 활용한 HTML 페이지 요청\n",
    "# 1-1) res 객체에 HTML 데이터가 저장되고, res.content로 데이터를 추출할 수 있음\n",
    "res = requests.get('http://v.media.daum.net/v/20170615203441266')\n",
    "# print(res.content)\n",
    "# 2) HTML 페이지 파싱 BeautifulSoup(HTML데이터, 파싱방법) # 2-1) BeautifulSoup\n",
    "#파싱방법\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "# 3) 필요한 데이터 검색\n",
    "title = soup.find('strong','tit_emotion')\n",
    "# 4) 필요한 데이터 추출\n",
    "print(title.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae6afd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔금대출에도 DTI 규제 적용 검토\n"
     ]
    }
   ],
   "source": [
    "data = soup.find('h3', 'tit_view') \n",
    "print(data.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "833ca916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김동욱\n"
     ]
    }
   ],
   "source": [
    "data = soup.find('span', 'txt_info') \n",
    "print(data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9445e92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김동욱\n",
      "입력 2017. 06. 15. 20:34\n",
      "수정 2017. 06. 15. 21:31\n"
     ]
    }
   ],
   "source": [
    "data = soup.find_all('span', 'txt_info')\n",
    "for item in data:\n",
    "    print(item.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59a67264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 2017. 06. 15. 20:34\n"
     ]
    }
   ],
   "source": [
    "data = soup.find_all('span', 'txt_info') \n",
    "print(data[1].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3538447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "금융당국이 급증하는 가계부채 증가세를 막기 위해 아파트 잔금대출에도 소득을 따져 대출한도를 정하는 총부채상환비율(DTI)을 적용하는 방안을 유력하게 검토하고 있다.\n",
      "지금은 집값을 기준으로 대출한도를 매기는 주택담보인정비율(LTV) 규제만 적용돼 소득이 없어도 집값의 70%를 빌려 잔금을 치르는 게 가능하다.\n",
      "앞으로 잔금대출에 DTI가 적용되면 소득이 없는 사람은 집값의 70% 대출 받는 게 어려워진다. 기사 제목과 주요 문장을 기반으로 자동요약한 결과입니다. 전체 맥락을 이해하기 위해서는 본문 보기를 권장합니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = soup.find('div', 'layer_util layer_summary') \n",
    "print(data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d099ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(왕초보) - 클래스 소개\n",
      "(왕초보) - 블로그 개발 필요한 준비물 준비하기\n",
      "(왕초보) - Github pages 설정해서 블로그 첫 페이지 만들어보기\n",
      "(왕초보) - 초간단 페이지 만들어보기\n",
      "(왕초보) - 이쁘게 테마 적용해보기\n",
      "(왕초보) - 마크다운 기초 이해하고, 실제 나만의 블로그 페이지 만들기\n",
      "(왕초보) - 다양한 마크다운 기법 익혀보며, 나만의 블로그 페이지 꾸며보기\n",
      "(초급) - 강사가 실제 사용하는 자동 프로그램 소개 [2]\n",
      "(초급) - 필요한 프로그램 설치 시연 [5]\n",
      "(초급) - 데이터를 엑셀 파일로 만들기 [9]\n",
      "(초급) -     엑셀 파일 이쁘게! 이쁘게! [8]\n",
      "(초급) -     나대신 주기적으로 파이썬 프로그램 실행하기 [7]\n",
      "(초급) - 파이썬으로 슬랙(slack) 메신저에 글쓰기 [40]\n",
      "(초급) - 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기 [12]\n",
      "(초급) - 네이버 API 사용해서, 블로그에 글쓰기 [42]\n",
      "(중급) - 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기 [412]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test')\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "titles = soup.find_all('li', 'course')\n",
    "for title in titles:\n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f3354d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(왕초보) - 클래스 소개\n",
      "(왕초보) - 블로그 개발 필요한 준비물 준비하기\n",
      "(왕초보) - Github pages 설정해서 블로그 첫 페이지 만들어보기\n",
      "(왕초보) - 초간단 페이지 만들어보기\n",
      "(왕초보) - 이쁘게 테마 적용해보기\n",
      "(왕초보) - 마크다운 기초 이해하고, 실제 나만의 블로그 페이지 만들기\n",
      "(왕초보) - 다양한 마크다운 기법 익혀보며, 나만의 블로그 페이지 꾸며보기\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test')\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "section = soup.find('ul', id='hobby_course_list')\n",
    "titles = section.find_all('li', 'course')\n",
    "for title in titles:\n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af789108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(초급) - 강사가 실제 사용하는 자동 프로그램 소개 [2]\n",
      "(초급) - 필요한 프로그램 설치 시연 [5]\n",
      "(초급) - 데이터를 엑셀 파일로 만들기 [9]\n",
      "(초급) -     엑셀 파일 이쁘게! 이쁘게! [8]\n",
      "(초급) -     나대신 주기적으로 파이썬 프로그램 실행하기 [7]\n",
      "(초급) - 파이썬으로 슬랙(slack) 메신저에 글쓰기 [40]\n",
      "(초급) - 웹사이트 변경사항 주기적으로 체크해서, 메신저로 알람주기 [12]\n",
      "(초급) - 네이버 API 사용해서, 블로그에 글쓰기 [42]\n",
      "(중급) - 자동으로 쿠팡파트너스 API 로 가져온 상품 정보, 네이버 블로그/트위터에 홍보하기 [412]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test')\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "section = soup.find('ul', id='dev_course_list')\n",
    "titles = section.find_all('li', 'course')\n",
    "for title in titles:\n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5fa6613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 클래스 소개\n",
      "2. 블로그 개발 필요한 준비물 준비하기\n",
      "3. Github pages 설정해서 블로그 첫 페이지 만들어보기\n",
      "4. 초간단 페이지 만들어보기\n",
      "5. 이쁘게 테마 적용해보기\n",
      "6. 마크다운 기초 이해하고, 실제 나만의 블로그 페이지 만들기\n",
      "7. 다양한 마크다운 기법 익혀보며, 나만의 블로그 페이지 꾸며보기\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://davelee-fun.github.io/blog/crawl_test')\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "section = soup.find('ul', id='hobby_course_list')\n",
    "titles = section.find_all('li', 'course')\n",
    "for index, title in enumerate(titles):\n",
    "    print(str(index+1)+'.',title.get_text().split('-')[1].split('[')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "723bbb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " The copyright belongs to the original writer of the content, and there may be errors in machine translation results. 版权归内容原作者所有。机器翻译结果可能存在错误。 原文の著作権は原著著作者にあり、機械翻訳の結果にエラーが含まれることがあります。 Hak cipta milik penulis asli dari konten, dan mungkin ditemukan kesalahan dalam hasil terjemahan mesin. Bản quyền thuộc về tác giả gốc của nội dung và có thể có lỗi trong kết quả dịch bằng máy.\n",
      "\n",
      "  한강공원 고 손정민씨 실종 둘러싼 타임라인정민씨 아버지\"친구 휴대폰 박살난 상태\" \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            지난달 25일 서울 반포한강공원 수상택시승강장 근처 잔디밭에서 대학 친구와 함께 술을 마시다가 실종된 뒤 닷새 만에 실종지점에서 싸늘한 주검으로 발견된 손정민(21) 씨의 빈소가 2일 서울 강남구 서울성모병원 장례식장에 차려져 있다.최영권 기자 story@seoul.co.kr\n",
      "           \n",
      "\n",
      "지난달 25일 서울 반포한강공원에서 실종된 대학생 손정민(21)씨가 닷새 만인 지난달 30일 시신으로 발견됐다.\n",
      "유족들은 시신의 머리 뒤쪽에 깊게 베인 상처 두 곳을 발견하고 경찰에 부검을 요청했다. 지난 1일 국립과학수사연구원이 부검을 했지만 시신의 부패가 진행돼 육안으로는 정확한 사인을 알 수 없었다. 시료의 정밀 분석에 착수했고, 결과는 보름 뒤쯤 나온다.\n",
      "경찰은 부검과 별개로 친구와 술을 마시다 잠든 정민씨가 숨진 경위를 밝히기 위해 수사력을 모으고 있다. 실종된 아들을 찾기 위해 블로그에 글을 올리고 전단을 돌렸던 아버지 손현(49)씨는 “아이 잃은 아빠는 더 이상 잃을 게 없다. 그 대가를 반드시 치르게 하겠다고 우리 아들에게 맹세했다”며 죽음의 이유를 끝까지 밝혀내겠다고 했다.\n",
      "신발은 버렸고, 휴대폰은 잃어버렸다\n",
      "대학교 1학년인 정민씨는 지난달 24일 오후 11시쯤부터 이튿날 오전 2시까지 한강공원에서 친구 A씨와 술을 마시고 잠들었다 실종됐다. A씨는 오전 4시30분쯤 혼자 집으로 돌아갔고, 당시 정민씨가 보이지 않아 먼저 귀가한 것으로 생각했다고 경찰에 진술했다.\n",
      "A씨는 오전 3시30분쯤 자신의 부모와 한 통화에서 정민씨가 취해 잠들었는데 깨울 수가 없다는 취지로 이야기한 것으로 전해졌다. A씨는 정민씨의 갤럭시 휴대전화를 가지고 귀가했다. 정작 A씨의 아이폰은 찾지 못했다. 4일 오후 정민씨 아버지는 A씨의 휴대폰을 찾았지만 박살이 난 상태라고 밝혔다. 경찰은 정민씨의 휴대전화를 포렌식하고, 지난달 25일 오전 3시를 전후해서 반포한강공원을 방문한 차량의 블랙박스를 조사하고 있다.\n",
      "\n",
      "\n",
      "\n",
      "            지난달 25일 서울 서초구 반포한강공원에서 실종됐던 대학생 손정민씨가 실종 닷새 만에 숨진 채 발견됐다. 경찰은 폐쇄회로(CC)TV 등 증거 확보에 애를 먹고 있다. 2일 한강공원 반포나들목에 설치된 CCTV 카메라가 시민들을 촬영하고 있다.박윤슬 기자 seul@seoul.co.kr\n",
      "           \n",
      "\n",
      "정민씨의 아버지가 언론에 나선 이유\n",
      "정민씨의 장례를 치르고 있는 아버지는 실종 당시부터 지금까지 경찰조사 과정에서 친구 A씨 측이 보인 행동에 의문점을 가지게 됐고, 언론에 나서서 그간의 일들을 세세하게 밝혔다.\n",
      "● 오전 2시부터 4시30분 정민씨의 행적 \n",
      "친구 A씨가 오전 3시30분에 그의 부모에게 통화한 사실을 정민씨의 아버지는 알지 못했다. 정민씨 아버지는 “새벽 2시부터 4시30분 사이에 무엇을 했냐고 물어봤는데 3명(A씨와 그의 가족) 모두 통화했다는 말을 한 적이 없다”고 말했다.\n",
      "A씨는 사건 당일 신었던 신발을 버렸다. ‘달리다가 넘어진 정민이를 부축해 일으키는 과정에서 자신의 옷과 신발이 더러워졌다’는 게 이유였다. 정민씨 아버지는 “그 아이(A씨) 아빠한테 신발이 좀 보고 싶다고 전화를 했는데 ‘버렸다’고 즉답이 나왔다. 아들의 행적을 추적하는데 바지와 옷이 더러워졌다고 강조했다. 그게 이상했다”고 말했다.\n",
      "A씨는 경찰 조사에 변호사를 대동했다. 최면 수사에서는 이렇다할 진술이 나오지 않은 것으로 알려졌다. 정민씨 아버지는 “최면은 당사자 의지가 있어야 하는데 정황을 들어보니 A씨는 숨기려 하기 때문에 최면이 안될 것이라는 말을 들었다”면서 “적극적으로 조사받아야 하는 애가 변호사를 데리고 왔다는 건 자기 방어를 해야 된다는 거다. 그 한 시간동안 무슨 일이 생겨서 우리 아들이 한강에 갔는지만 알면 모든 원한이 풀린다”고 강조했다.\n",
      "\n",
      "\n",
      "\n",
      "            한강 실종 대학생 진상규명 국민청원\n",
      "           \n",
      "\n",
      "● 정민씨의 부모에겐 연락하지 않은 친구\n",
      "친구 A씨가 자신의 부모와 통화를 했던 3시30분쯤, 정작 정민씨의 아버지는 어떠한 연락도 받지 못했다. 정민씨의 아버지는 “5시가 넘어도 나와 아내에게 연락을 하지 않은 데에 대한 사과는 해야 한다”고 강조했다.\n",
      "정민씨의 친구는 4일 새벽 작은 아버지와 함께 장례식장을 찾았다. 빈소가 마련된 지 닷새만이었다.\n",
      "정민씨의 아버지에 따르면 A씨의 작은 아버지는 조카가 많이 힘들어한다며 새벽 1시30분 빈소를 찾았다. 정민씨의 아버지는 “아무도 없을 때 조문을 온 것 같다. 부모는 얼굴도 못 내밀고 친척을 앞세워 왔다. 늦었으니 나가라고 했다”고 말했다.\n",
      "\n",
      "\n",
      "\n",
      "            강남세브란스병원이 4일 페이스북에 게재한 게시물. 사진=페이스북\n",
      "           \n",
      "\n",
      "● A씨의 집안배경 둘러싼 루머들\n",
      "친구 A씨의 집안배경을 두고 여러 루머들이 나오고 있다. A씨의 아버지는 경찰 또는 변호사가 아닌 것으로 밝혀졌다. 강남세브란스병원 교수의 아들이란 소문도 있었지만 병원 측이 직접 “사실과 다르다”고 입장을 밝혔다.\n",
      "정민씨가 실종된 지난달 25일 0시부터 새벽 4시까지 사건 현장 인근 서래섬에서 낚시하던 남성이 ‘인근에 경찰차 6대가 출동했다’고 올린 목격담은 정민씨의 실종과는 연관이 없는 것으로 확인됐다. 경찰은 당시 한강변 식당 건물 주차장에서 차량 접촉사고가 발생해 출동했다고 밝혔다.\n",
      "인근 편의점 폐쇄회로(CC)TV에 달려가는 모습이 포착된 남성 3명은 조사결과 고교생 1명과 중학생 2명으로 동네 선후배 사이일뿐 사건과 관련이 없는 것으로 밝혀졌다.\n",
      "김유민 기자 planet@seoul.co.kr\n",
      "\n",
      "Copyrightsⓒ 서울신문사. 무단 전재 및 재배포 금지 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 이 기사에 대해 어떻게 생각하시나요?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  연재  \n",
      " 더보기\n",
      "\n",
      "\n",
      "\n",
      "서울신문 주요 뉴스\n",
      "해당 언론사로 연결됩니다.\n",
      "\n",
      "\n",
      "폐지 주우려 리어카 끌다가… 외제차 긁어 벌금형\n",
      "“사랑하셔서 그래요” 주먹질당한 연인에 ‘또 한방’ 먹인 경찰\n",
      "“임신한 줄 몰랐다” 하와이행 비행기 화장실서 20대 출산\n",
      "“바람 쐬고 싶다”며 남편 차에서 내렸는데…50대 여성 추락사\n",
      "김부겸 부부, 자동차세·과태료 체납… 차량 32번 압류\n",
      "한강 실종 대학생 父 “아들 스스로 뛰어들리 없다”\n",
      "“바람 못 피우게…” 남편 밥에 여성호르몬 섞는 中 아내들\n",
      "“전교 1등이라 봐줬다?” 강남 뒤흔든 부정행위 의혹 [이슈픽]\n",
      "“기어들어와라”…누나 살해 남동생, 경찰관도 속인 카톡\n",
      "배고프니까 청춘이다… ‘3000원 밥상’ 차린 신부님\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "많이본 뉴스\n",
      "\n",
      " 뉴스 \n",
      " 연예 \n",
      " 스포츠 \n",
      "\n",
      "\n",
      "\n",
      "포토&TV\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://news.v.daum.net/v/20210504133101298')\n",
    "soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "section = soup.find('div', id='cMain')\n",
    "titles = section.find('div', 'p')\n",
    "print(section.text)\n",
    "#print(titles)\n",
    "#for index, title in enumerate(titles):\n",
    "    #print(str(index+1)+'.',title.get_text().split('-')[1].split('[')[0].strip())\n",
    "#for title in titles:\n",
    "    #print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829e16b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
